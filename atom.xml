<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>软件测试技术分享</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-23T06:57:00.069Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>yulong</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(10)/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(10)/</id>
    <published>2022-05-23T06:57:08.658Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(9)/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(9)/</id>
    <published>2022-05-23T06:57:08.183Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(8)/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(8)/</id>
    <published>2022-05-23T06:57:07.991Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(7)/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(7)/</id>
    <published>2022-05-23T06:57:07.762Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(6)/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(6)/</id>
    <published>2022-05-23T06:57:07.386Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(5)/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC%20(5)/</id>
    <published>2022-05-23T06:57:07.265Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>scrapy_gerapy爬虫管理框架</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05_scrapy_gerapy%E7%88%AC%E8%99%AB%E7%AE%A1%E7%90%86%E6%A1%86%E6%9E%B6/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05_scrapy_gerapy%E7%88%AC%E8%99%AB%E7%AE%A1%E7%90%86%E6%A1%86%E6%9E%B6/</id>
    <published>2022-05-23T06:57:07.077Z</published>
    <updated>2022-05-27T07:09:56.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-gerapy介绍"><a href="#1-gerapy介绍" class="headerlink" title="1. gerapy介绍"></a>1. gerapy介绍</h2><p>gerapy 是一款分布式爬虫管理框架，支持Python3，基于scrapy、scrapyd<br>scrapyd-client、scrapy-redis、scrapyd-api、scrapy-splash、jinjia2、django、vue.js开发、gerapy可以帮助我们：<br>    1. 更方便的控制爬虫运行<br>    2. 更直观的查看爬虫状态<br>    3. 更实时的查看爬去结果<br>    4. 更简单的实现项目部署<br>    5. 更统一地实现主机管理</p><h2 id="2-gerapy的安装"><a href="#2-gerapy的安装" class="headerlink" title="2. gerapy的安装"></a>2. gerapy的安装</h2><pre><code>1. 执行如下命令，等待安装完毕    pip install gerapy2. 验证gerapy是否安装成功    在终端中执行gerapy 会出现如下信息    ![wer](2232)3. gerapy 配置启动    1. 新建一个项目        gerapy init        执行完该命令之后在当前目录下生成一个gerapy文件夹，进入该文件夹，会找到一个名为projects 的文件    2. 对数据库进行初始化（在gerapy目录中操作），执行如下命令        gerapy migrate        ![image.png](assets/2/image-20220527144809-zun804x.png)        对数据库初始化之后会生成一个SQLite数据库，数据库保存主机配置和部署版本等        ![image.png](assets/2/image-20220527144834-crdotse.png)    3. 启动gerapy服务        此时启动gerapy服务的这台机器的8000端口上开启了gerapy服务，在浏览器中输入http://localhost:8000就能进入gerapy管理界面，在管理界面就可以进行主机管理和界面管理        ![image.png](assets/2/image-20220527145151-fkkxt6b.png)    4. 通过gerapy配置管理scrapy项目    5. 配置project        1. 我们可以将scrapy项目直接放到/gerapy/projects下        2. 可以在gerapy后台看到有个项目        ![image.png](assets/2/image-20220527145500-k99cqu4.png)        3. 点击部署点击部署按钮进行打包和部署，在右下角我么可以输入打包时的描述信息，类似于git的commit信息，        ![image.png](assets/2/image-20220527145743-ls1oklb.png)        然后点击打包按钮，即可发现gerapy会提示打包成功，同时在右侧显示打包的结果和打包名称        ![image.png](assets/2/image-20220527145820-tdw8if1.png)        4. 选择一个站点，点击右侧部署，将该项目部署到该站点上        ![image.png](assets/2/image-20220527150104-my849n7.png)        5. 成功部署之后会显示描述和部署时间        ![image.png](assets/2/image-20220527150307-9u2rn2l.png)        6. 来到client界面，找到部署该项目的节点，点击调度        ![image.png](assets/2/image-20220527150412-68g8gf6.png)        7. 在该节点中的项目列表中找到项目，点击右侧run运行项目        ![image.png](assets/2/image-20220527150528-q4ueh1u.png)    6. 补充        1.gerapy 与scrad有什么联系            我们仅仅使用scrapyd 是可以调用scrapy进行爬虫，只是需要使用命令进行开启爬虫 curl http://127.0.0.1:6800/schedule.json -d project=工程名 -d spider=爬虫名 使用gerapy就是将使用命令进行开启爬虫变成“小手一点”，我们在gerapy中配置scrad后，不需要使用命令行，可以通过图形化界面直接开启爬虫</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-gerapy介绍&quot;&gt;&lt;a href=&quot;#1-gerapy介绍&quot; class=&quot;headerlink&quot; title=&quot;1. gerapy介绍&quot;&gt;&lt;/a&gt;1. gerapy介绍&lt;/h2&gt;&lt;p&gt;gerapy 是一款分布式爬虫管理框架，支持Python3，基于scra</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>scrapy_scrapyd部署</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04_scrapy_scrapyd%E9%83%A8%E7%BD%B2/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04_scrapy_scrapyd%E9%83%A8%E7%BD%B2/</id>
    <published>2022-05-23T06:57:06.809Z</published>
    <updated>2022-05-27T06:32:45.789Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-scrapyd的介绍"><a href="#1-scrapyd的介绍" class="headerlink" title="1. scrapyd的介绍"></a>1. scrapyd的介绍</h3><p>scrapyd是一个用于部署和运行scrapy爬虫的程序，它允许你通过json api 来部署爬虫项目和控制爬虫运行，scrapyd 是一个守护进程，监听爬虫的运行和请求，然后启动进程来执行他们</p><blockquote><p>所谓json api本质就是post请求的webapi</p></blockquote><h3 id="2-scrapyd的安装"><a href="#2-scrapyd的安装" class="headerlink" title="2. scrapyd的安装"></a>2. scrapyd的安装</h3><p>scrapyd 服务 ： pip install scrapyd<br>scrapyd 客户端： pip install scrapyd-client</p><h3 id="3-启动scrapyd服务"><a href="#3-启动scrapyd服务" class="headerlink" title="3. 启动scrapyd服务"></a>3. 启动scrapyd服务</h3><ol><li>在scrapy项目路径下 启动scrapyd 的命令： sudo scrapyd 或scrapyd</li><li>启动之后可以在本地打开运行的scrapyd，浏览器中访问本地6800端口可以查看scrapyd 的监控界面</li><li><img src="/111" alt="为配图，待截图"></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-scrapyd的介绍&quot;&gt;&lt;a href=&quot;#1-scrapyd的介绍&quot; class=&quot;headerlink&quot; title=&quot;1. scrapyd的介绍&quot;&gt;&lt;/a&gt;1. scrapyd的介绍&lt;/h3&gt;&lt;p&gt;scrapyd是一个用于部署和运行scrapy爬虫的程序</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>03_scrapy_splash框架问题分析</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03_scrapy_splash%E6%A1%86%E6%9E%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03_scrapy_splash%E6%A1%86%E6%9E%B6%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</id>
    <published>2022-05-23T06:57:06.429Z</published>
    <updated>2022-05-26T13:23:40.642Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-sittings-py-文件配置："><a href="#1-sittings-py-文件配置：" class="headerlink" title="1. sittings.py 文件配置："></a>1. sittings.py 文件配置：</h2><h5 id="1-修改settings-py，配置SPLASH-URL。在这里我们的Splash是在本地运行的，所以可以直接配置本地的地址："><a href="#1-修改settings-py，配置SPLASH-URL。在这里我们的Splash是在本地运行的，所以可以直接配置本地的地址：" class="headerlink" title="1. 修改settings.py，配置SPLASH_URL。在这里我们的Splash是在本地运行的，所以可以直接配置本地的地址："></a>1. 修改settings.py，配置SPLASH_URL。在这里我们的Splash是在本地运行的，所以可以直接配置本地的地址：</h5><pre><code>SPLASH_URL = &#39;http://localhost:8050&#39;</code></pre><h5 id="如果Splash是在远程服务器运行的，那此处就应该配置为远程的地址。例如运行在IP为120-27-34-25的服务器上，则此处应该配置为："><a href="#如果Splash是在远程服务器运行的，那此处就应该配置为远程的地址。例如运行在IP为120-27-34-25的服务器上，则此处应该配置为：" class="headerlink" title="如果Splash是在远程服务器运行的，那此处就应该配置为远程的地址。例如运行在IP为120.27.34.25的服务器上，则此处应该配置为："></a>如果Splash是在远程服务器运行的，那此处就应该配置为远程的地址。例如运行在IP为120.27.34.25的服务器上，则此处应该配置为：</h5><pre><code>SPLASH_URL = &#39;http://120.27.34.25:8050&#39;</code></pre><h5 id="2-还需要配置几个Middleware，代码如下所示："><a href="#2-还需要配置几个Middleware，代码如下所示：" class="headerlink" title="2. 还需要配置几个Middleware，代码如下所示："></a>2. 还需要配置几个Middleware，代码如下所示：</h5><pre><code>DOWNLOADER_MIDDLEWARES = &#123;   &#39;scrapy_splash.SplashCookiesMiddleware&#39;: 723,   &#39;scrapy_splash.SplashMiddleware&#39;: 725,   &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 810,&#125;SPIDER_MIDDLEWARES = &#123;   &#39;scrapy_splash.SplashDeduplicateArgsMiddleware&#39;: 100,&#125;</code></pre><p>这里配置了三个Downloader Middleware和一个Spider Middleware，这是Scrapy-Splash的核心部分。我们不再需要像对接Selenium那样实现一个Downloader Middleware，Scrapy-Splash库都为我们准备好了，直接配置即可。</p><h5 id="3-还需要配置一个去重的类DUPEFILTER-CLASS，代码如下所示："><a href="#3-还需要配置一个去重的类DUPEFILTER-CLASS，代码如下所示：" class="headerlink" title="3. 还需要配置一个去重的类DUPEFILTER_CLASS，代码如下所示："></a>3. 还需要配置一个去重的类DUPEFILTER_CLASS，代码如下所示：</h5><pre><code>DUPEFILTER_CLASS = &#39;scrapy_splash.SplashAwareDupeFilter&#39;1</code></pre><h5 id="4-最后配置一个Cache存储HTTPCACHE-STORAGE，代码如下所示："><a href="#4-最后配置一个Cache存储HTTPCACHE-STORAGE，代码如下所示：" class="headerlink" title="4. 最后配置一个Cache存储HTTPCACHE_STORAGE，代码如下所示："></a>4. 最后配置一个Cache存储HTTPCACHE_STORAGE，代码如下所示：</h5><pre><code>HTTPCACHE_STORAGE = &#39;scrapy_splash.SplashAwareFSCacheStorage&#39;</code></pre><h2 id="2-使用过程中遇到的问题分析"><a href="#2-使用过程中遇到的问题分析" class="headerlink" title="2. 使用过程中遇到的问题分析"></a>2. 使用过程中遇到的问题分析</h2><h3 id="1-错误演示-如图"><a href="#1-错误演示-如图" class="headerlink" title="1. 错误演示  如图"></a>1. 错误演示  如图</h3><p>spider文件编写代码错误</p><pre><code>    def start_requests(self):         yield SplashRequest(             url=self.start_urls[0],       # 这是一个错误的演示， 必须使用for循环取值，不能使用下标获取值，使用下标的话 会报 404的错误，找不到原因             callback=self.parse,             args=&#123;&quot;wait&quot;:10&#125;, # 最大超时时间，单位：s             endpoint=&quot;reader.html&quot;         )</code></pre><p><img src="/assets/2/image-20220526211507-asoegu2.png" alt="image.png"></p><h4 id="正确演示："><a href="#正确演示：" class="headerlink" title="正确演示："></a>正确演示：</h4><pre><code>    def start_requests(self):         for start_url in self.start_urls:             yield SplashRequest(start_url,                                 callback=self.parse,                                 args=&#123;&#39;wait&#39;: 10&#125;,  # 最大超时时间，单位：秒                                 endpoint=&#39;render.html&#39;)  # 使用splash服务的固定参数</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-sittings-py-文件配置：&quot;&gt;&lt;a href=&quot;#1-sittings-py-文件配置：&quot; class=&quot;headerlink&quot; title=&quot;1. sittings.py 文件配置：&quot;&gt;&lt;/a&gt;1. sittings.py 文件配置：&lt;/h2&gt;&lt;h5</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_课程学习笔记</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E5%89%AF%E6%9C%AC/</id>
    <published>2022-05-23T06:57:05.495Z</published>
    <updated>2022-05-23T06:57:00.069Z</updated>
    
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_概念和流程</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01_scrapy_%E6%A6%82%E5%BF%B5%E5%92%8C%E6%B5%81%E7%A8%8B/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01_scrapy_%E6%A6%82%E5%BF%B5%E5%92%8C%E6%B5%81%E7%A8%8B/</id>
    <published>2022-05-23T06:56:21.085Z</published>
    <updated>2022-05-23T06:58:05.331Z</updated>
    
    <content type="html"><![CDATA[<h1 id="scrapy-的概念和流程"><a href="#scrapy-的概念和流程" class="headerlink" title="scrapy 的概念和流程"></a>scrapy 的概念和流程</h1><h2 id="第一讲"><a href="#第一讲" class="headerlink" title="第一讲"></a>第一讲</h2><h3 id="学习目的"><a href="#学习目的" class="headerlink" title="学习目的"></a>学习目的</h3><ol><li>了解scrapy的概念</li><li>了解dcrapy框架的作用</li><li>掌握scrapy框架的运行流程</li><li>掌握scrapy中的每个模块的作用</li></ol><h3 id="1-scrapy的概念"><a href="#1-scrapy的概念" class="headerlink" title="1.scrapy的概念"></a>1.scrapy的概念</h3><h4 id="scrapy是一个python编写的开源网络爬虫框架，他是一个被设计-用于爬取网络数据、提取结构性数据的框架"><a href="#scrapy是一个python编写的开源网络爬虫框架，他是一个被设计-用于爬取网络数据、提取结构性数据的框架" class="headerlink" title="scrapy是一个python编写的开源网络爬虫框架，他是一个被设计 用于爬取网络数据、提取结构性数据的框架"></a>scrapy是一个python编写的开源网络爬虫框架，他是一个被设计 用于爬取网络数据、提取结构性数据的框架</h4><pre><code> scrapy使用了twisted异步网络框架，可以加快我们的下载速度</code></pre><p>scrapy文档：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/">https://scrapy-chs.readthedocs.io/zh_CN&#x2F;0.24&#x2F;</a></p><h3 id="2-scrapy框架的作用"><a href="#2-scrapy框架的作用" class="headerlink" title="2.scrapy框架的作用"></a>2.scrapy框架的作用</h3><h4 id="少量的代码，就能够快速的抓取"><a href="#少量的代码，就能够快速的抓取" class="headerlink" title="少量的代码，就能够快速的抓取"></a>少量的代码，就能够快速的抓取</h4><h3 id="3-scrapy的工作流程"><a href="#3-scrapy的工作流程" class="headerlink" title="3.scrapy的工作流程"></a>3.scrapy的工作流程</h3><h4 id="3-1-回顾之前的爬虫流程"><a href="#3-1-回顾之前的爬虫流程" class="headerlink" title="3.1 回顾之前的爬虫流程"></a>3.1 回顾之前的爬虫流程</h4><p> <img src="/assets/2/image-20220523141200-qq1ar39.png" alt="image.png"></p><h4 id="3-2-上面流程可以修改为"><a href="#3-2-上面流程可以修改为" class="headerlink" title="3.2 上面流程可以修改为"></a>3.2 上面流程可以修改为</h4><p> <img src="/assets/2/image-20220523142540-r3yhln7.png" alt="image.png"></p><h4 id="3-3-scrapy-的流程"><a href="#3-3-scrapy-的流程" class="headerlink" title="3.3 scrapy 的流程"></a>3.3 scrapy 的流程</h4><p><img src="/assets/2/scrapy1.jpg" alt="image.png"><br><img src="/assets/2/image-20220523142912-6lkduyf.png" alt="image.png"><br>其流程可以描述如下：<br>    1. 爬虫中起始的url构造成request对象–&gt;爬虫中间件–&gt;迎请–&gt;调度器<br>    2.  调度器把request–&gt;迎请–&gt;下载中间件–&gt;下载器<br>    3.  下载器发送请求，获取response 响应–&gt;下载中间件–&gt;引擎–&gt;爬虫<br>    4.  爬虫提取url地址，组装成request对象–&gt;爬虫中间件–&gt;引擎–&gt;调度器，重复步骤2<br>    5.  爬虫提取数据–&gt;引擎–&gt;管道处理和保存数据<br>注意：<br>    * 图中中文是为了方便理解后加上去的<br>    * 图中绿色线条的表示数据的传递<br>    * 注意中间件的位置，决定其作用<br>    * 注意其中引擎的位置，所有的模块之间相互独立，只和引擎进行交互</p><h4 id="3-4-scrapy的三个内组织对象"><a href="#3-4-scrapy的三个内组织对象" class="headerlink" title="3.4 scrapy的三个内组织对象"></a>3.4 scrapy的三个内组织对象</h4><pre><code>* request请求对象：由url method post_data headers 等构成* response响应对象：由url body status headers等构成* item数据对象：本质是个字典</code></pre><h4 id="3-5-scrapy中的每个模块的具体作用"><a href="#3-5-scrapy中的每个模块的具体作用" class="headerlink" title="3.5 scrapy中的每个模块的具体作用"></a>3.5 scrapy中的每个模块的具体作用</h4><p>![image.png](assets&#x2F;2&#x2F;  image-20220523144758-47dlg2q.png)<br>注意：<br>    * 爬虫中间件和下载中间件知识运行逻辑的位置不同，作用是重复的：如替换UA等</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;scrapy-的概念和流程&quot;&gt;&lt;a href=&quot;#scrapy-的概念和流程&quot; class=&quot;headerlink&quot; title=&quot;scrapy 的概念和流程&quot;&gt;&lt;/a&gt;scrapy 的概念和流程&lt;/h1&gt;&lt;h2 id=&quot;第一讲&quot;&gt;&lt;a href=&quot;#第一讲&quot; c</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python第-scrapy框架_入门使用</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02_scrapy_%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8/</id>
    <published>2022-05-23T06:02:15.176Z</published>
    <updated>2022-05-26T13:10:16.665Z</updated>
    
    <content type="html"><![CDATA[<h2 id="学习目标："><a href="#学习目标：" class="headerlink" title="学习目标："></a>学习目标：</h2><pre><code>1. 掌握scrapy的安装2. 应用创建scrapy的项目3. 应用创建scrapy爬虫4. 应用运行scrapy爬虫5. 应用scrapy定位以及提取数据或属性值得方法6. 掌握response响应对象的常用属性</code></pre><h2 id="1-安装scrapy"><a href="#1-安装scrapy" class="headerlink" title="1. 安装scrapy"></a>1. 安装scrapy</h2><h4 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h4><blockquote><p>sudo apt-get install scrapy</p></blockquote><h4 id="或者"><a href="#或者" class="headerlink" title="或者"></a>或者</h4><blockquote><p>pip&#x2F;pip3 install  scrapy</p></blockquote><h2 id="2-scrapy-项目开发流程"><a href="#2-scrapy-项目开发流程" class="headerlink" title="2. scrapy 项目开发流程"></a>2. scrapy 项目开发流程</h2><pre><code>1.创建项目：    scrapy startoject mySpider2.生成一个爬虫    scrapy genspider itcast itcast.cn3.提取数据    根据网站结果在spider中实现数据采集相关内容4.保存数据    使用pipeline进行数据后续处理和保存</code></pre><h2 id="3-创建项目"><a href="#3-创建项目" class="headerlink" title="3. 创建项目"></a>3. 创建项目</h2><blockquote><p>通过命令将scrapy项目的文件生成出来，后续步骤都是在项目文件中进行相关操作，下面抓取传智师资库来学习scrapy的入门使用：<a href="http://www.itcast.cn/channel/teacher.shtml">http://www.itcast.cn/channel/teacher.shtml</a><br>创建scrapy项目的命令：<br>scrapy startpoject &lt;项目名称&gt;<br>示例：<br>scrapy startproject myspider<br>生成的目录和文件结构如图：<br><img src="/assets/image-20220523151330-kj8lufa.png" alt="image.png"></p></blockquote><h2 id="4-创建爬虫"><a href="#4-创建爬虫" class="headerlink" title="4. 创建爬虫"></a>4. 创建爬虫</h2><h3 id="通过命令创建出爬虫文件，爬虫文件为主要的代码作业文件通常一个网站的爬虫动作都会在爬虫文件中进行编写"><a href="#通过命令创建出爬虫文件，爬虫文件为主要的代码作业文件通常一个网站的爬虫动作都会在爬虫文件中进行编写" class="headerlink" title="通过命令创建出爬虫文件，爬虫文件为主要的代码作业文件通常一个网站的爬虫动作都会在爬虫文件中进行编写"></a>通过命令创建出爬虫文件，爬虫文件为主要的代码作业文件通常一个网站的爬虫动作都会在爬虫文件中进行编写</h3><h4 id="命令：-1"><a href="#命令：-1" class="headerlink" title="命令："></a>命令：</h4><pre><code>在项目路径下执行：    &gt; scrapy genspider &lt;爬虫名字&gt;&lt;允许爬取的域名&gt;</code></pre><h4 id="爬虫名字：作为爬虫运行时的参数"><a href="#爬虫名字：作为爬虫运行时的参数" class="headerlink" title="爬虫名字：作为爬虫运行时的参数"></a>爬虫名字：作为爬虫运行时的参数</h4><h4 id="允许爬取的域名：为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不同则被过滤掉"><a href="#允许爬取的域名：为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不同则被过滤掉" class="headerlink" title="允许爬取的域名：为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不同则被过滤掉"></a>允许爬取的域名：为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不同则被过滤掉</h4><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><blockquote><p>cd myspider<br>scrapy genspider itcast  itcast.cn</p></blockquote><p>生成的目录和文件结果如下：<br><img src="/assets/2/image-20220523153511-rqee69q.png" alt="image.png"></p><h3 id="5-完善爬虫"><a href="#5-完善爬虫" class="headerlink" title="5. 完善爬虫"></a>5. 完善爬虫</h3><blockquote><p>“””<br>filename:itcast.py<br>“””<br>import scrapy</p></blockquote><blockquote><p>class ItcastSpider(scrapy.Spider):</p><h1 id="爬虫名称"><a href="#爬虫名称" class="headerlink" title="爬虫名称"></a>爬虫名称</h1><p>   name &#x3D; ‘itcast’</p><h1 id="允许爬取的范围"><a href="#允许爬取的范围" class="headerlink" title="允许爬取的范围"></a>允许爬取的范围</h1><p>   allowed_domains &#x3D; [‘itcast.cn’]</p><h1 id="开始爬取的url地址"><a href="#开始爬取的url地址" class="headerlink" title="开始爬取的url地址"></a>开始爬取的url地址</h1><h1 id="start-urls-x3D-‘http-itcast-cn-39"><a href="#start-urls-x3D-‘http-itcast-cn-39" class="headerlink" title="start_urls &#x3D; [‘http://itcast.cn/&#39;]"></a>start_urls &#x3D; [‘<a href="http://itcast.cn/&#39;]">http://itcast.cn/&#39;]</a></h1><h1 id="修改开始爬取的地址"><a href="#修改开始爬取的地址" class="headerlink" title="修改开始爬取的地址"></a>修改开始爬取的地址</h1><p>   start_urls &#x3D; [‘<a href="http://www.itcast.cn/channel/teacher.shtml#ajavaee&#39;]">http://www.itcast.cn/channel/teacher.shtml#ajavaee&#39;]</a></p></blockquote><blockquote><h1 id="数据提取的方法，接受下载中间件传过来的response"><a href="#数据提取的方法，接受下载中间件传过来的response" class="headerlink" title="数据提取的方法，接受下载中间件传过来的response"></a>数据提取的方法，接受下载中间件传过来的response</h1><p>   def parse(self, response):<br>       #  获取所有教师节点<br>       node_list &#x3D; response.xpath(‘&#x2F;&#x2F;div[@class &#x3D; “li_txt”]’)<br>       # 遍历所有教师节点<br>       for  node   in   node_list:<br>           temp &#x3D; {}<br>           # xpath 方法返回的是选择器对象列表，<br>           # extract()用于从选择器兑现中提取数据<br>           #extract_first()  就不需要前面的索引[0],可能返回空列表可以处理，给None，列表只有一个值时使用<br>           temp[“name”] &#x3D; node.xpath(‘.&#x2F;h3&#x2F;text()’)[0].extract()<br>           temp[“title”] &#x3D; node.xpath(‘.&#x2F;h4&#x2F;text()’)[0].extract()<br>           temp[“desc”] &#x3D; node.xpath(‘.&#x2F;p&#x2F;text()’)[0].extract()<br>           # xpath 结果为只有一个值得列表可以使用extract_first()<br>           # xpath 结果如果有多个值时使用extract()<br>           yield  temp</p></blockquote><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><pre><code>* scrapy.Spider爬虫类中必须有名为parse的解析* 如果网站结果层次比较负载，也可以自定义其他解析函数* 在解析函数中提取的url地址如果要发送请求，则必须属于allowed_domains范围内，但是start_urls中的url地址不受这个限制，我们会在后续课程中学习如何在解析函数中构造发送请求* 启动爬虫的时候注意启动的位置，是在项目路径下启动* parse()函数中使用yield返回数据，注意：解析函数中的yield能够传递对象只能是：Baseltem，Request，dict，None</code></pre><h4 id="5-2-定位元素以及提取数据、属性值的方法"><a href="#5-2-定位元素以及提取数据、属性值的方法" class="headerlink" title="5.2 定位元素以及提取数据、属性值的方法"></a>5.2 定位元素以及提取数据、属性值的方法</h4><blockquote><p>解析并获取scrapy爬虫中的数据，利用xpath规则字符串进行定位和提取<br>    1. response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样<br>    2. 额外方法extract():返回一个包含字符串的列表<br>    3. 额外方法extract_first():返回列表中的第一个字符串，列表为空返回None</p></blockquote><h4 id="5-3-response响应对象的常见属性"><a href="#5-3-response响应对象的常见属性" class="headerlink" title="5.3 response响应对象的常见属性"></a>5.3 response响应对象的常见属性</h4><pre><code>* response.url：当前响应的url地址* response.request.url：当前响应对应的请求的url地址* response.headers：响应头* response.requests.headers:当前响应的请求头* response.body ：响应体，也就是html代码，byte类型</code></pre><h3 id="7-运行scrapy"><a href="#7-运行scrapy" class="headerlink" title="7. 运行scrapy"></a>7. 运行scrapy</h3><h4 id="命令：在项目目录下执行scrapy-crawl-lt-爬虫名字-gt"><a href="#命令：在项目目录下执行scrapy-crawl-lt-爬虫名字-gt" class="headerlink" title="命令：在项目目录下执行scrapy crawl&lt;爬虫名字&gt;"></a>命令：在项目目录下执行scrapy crawl&lt;爬虫名字&gt;</h4><blockquote><p>scrapy crawl itcast</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;学习目标：&quot;&gt;&lt;a href=&quot;#学习目标：&quot; class=&quot;headerlink&quot; title=&quot;学习目标：&quot;&gt;&lt;/a&gt;学习目标：&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;1. 掌握scrapy的安装
2. 应用创建scrapy的项目
3. 应用创建scrapy爬虫
4.</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    <category term="学习笔记" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter Notebook安装（Windows）</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/03_python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/02_python%E6%8F%92%E4%BB%B6_jupyter/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/03_python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/02_python%E6%8F%92%E4%BB%B6_jupyter/</id>
    <published>2022-05-20T18:07:37.681Z</published>
    <updated>2022-05-20T18:08:36.695Z</updated>
    
    <content type="html"><![CDATA[<p>Jupyter Notebook安装（Windows）</p><ol><li><p>下载Jupyter Notebook<br> （1）打开cmd（如果没有把Python安装目录添加到Path，就需要切换到Python安装目录的Scripts目录下，不过大多数的Python安装教程都会有这一步）；</p><p> <img src="/assets/2/image-20220521020425-aftpk03.png" alt="image.png"></p></li></ol><p>（2）输入pip install jupyter；</p><ol start="2"><li><p>启动Juypter Notebook<br> （1）命令行窗口输入jupyter notebook；</p><p> <img src="/assets/2/image-20220521020437-ydugrs8.png" alt="image.png"></p></li></ol><p>同时，默认浏览器会打开Jupyter Notebook窗口，说明Jupyter Notebook安装成功。</p><p><img src="/assets/2/image-20220521020447-b4gi4oq.png" alt="image.png"></p><ol start="3"><li><p>配置Jupyter Notebook<br> （1）命令行窗口输入jupyter notebook –generate-config，会发现C:\Users\用户名\ .jupyter下多出了一个配置文件jupyter_notebook_config.py；</p><p> <img src="/assets/2/image-20220521020459-2dhd2aa.png" alt="image.png"></p></li></ol><p>（2）打开这个配置文件，找到下面这句#c.NotebookApp.notebook_dir &#x3D; ‘’。</p><p><img src="/assets/2/image-20220521020506-0fcek2s.png" alt="image.png"></p><p>可以把它修改成c.NotebookApp.notebook_dir &#x3D; ‘D:\jupyter-notebook’，当然具体的目录由自己创建的文件夹决定（需要自己创建）。</p><p><img src="/assets/2/image-20220521020519-vll8ozo.png" alt="image.png"></p><p>配置文件修改完成后，以后在jupyter notebook中写的代码都会保存在该目录下。现在重新启动jupyter notebook，就进入了新的工作目录；</p><p><img src="/assets/2/image-20220521020528-wexy3m4.png" alt="image.png"></p><ol start="4"><li>添加代码自动补全功能（可选）<br> （1）打开cmd，输入pip install jupyter_contrib_nbextensions，等待安装成功；</li></ol><p>（2）安装完之后需要配置nbextension（配置前要确保已关闭jupyter notebook），在cmd中输入jupyter contrib nbextension install –user –skip-running-check，等待配置成功；</p><p>（3）在前两步成功的情况下，启动jupyter notebook，会发现在选项栏中多出了Nbextension的选项，点开该选项，并勾选Hinterland，即可添加代码自动补全功能。</p><p><img src="/assets/2/image-20220521020535-pwhm2y0.png" alt="image.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Jupyter Notebook安装（Windows）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载Jupyter Notebook&lt;br&gt; （1）打开cmd（如果没有把Python安装目录添加到Path，就需要切换到Python安装目录的Scripts目录下，不过大多数的Pyth</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="03_python数据分析" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/03-python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="课后" scheme="http://example.com/tags/%E8%AF%BE%E5%90%8E/"/>
    
  </entry>
  
  <entry>
    <title>python数据分析入门实例-pandas_matplotlib</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/03_python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/01_pandas_matplotlib/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/03_python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/01_pandas_matplotlib/</id>
    <published>2022-05-20T16:49:23.564Z</published>
    <updated>2022-05-20T17:42:24.317Z</updated>
    
    <content type="html"><![CDATA[<pre><code># 1. 读取csv文件道程序中，用一个合适的变量存储它# 2. 清洗，处理# 3. 可视化，画图# 安装 第三方库   jupyter# series  一维数据,dataframe  二维数据（csv）# 打开jupyter服务import pandas as pddf = pd.read_csv(&quot;文件名.csv&quot;)  #  读取所有的数据type(df)# pandas.core.frame.DateFramedf# 表格文件df.info()    # 文件属性df.columns   # 获取所有的列名# 删除无用列del(df[&quot;需要删除的列名&quot;]) # 删除原数据   df.列名(列名不能包含空格)dfhead()# df.drop(删除一行或者一列) df.drop_duplicates(删除重复数据)  df.dropna(删除空白数据) # df.drop([&quot;列名1&quot;,&quot;列名2&quot;]，axis = 1,inplace = True)# 一列表的形式删除列    inplace 是否在原数据上执行df.columns   # 获取所有的列名df.columns = [要修改的列名]   # 修改列名df.place #  假设此列为出版地# 先分组，然后数每组有多少数据count_df = df.groupby([&quot;place&quot;,&quot;date&quot;])    # 分组place_count = count_df[&quot;Identifier&quot;].count()  # 统计place_sorted = place_count.sort_values(ascending = False)    # False  降序   True  升序# 数据可视化，画图# 画什么图？？# 趋势，规律  画线图# 分布占比图  饼图@matplotlib inlineimport matplotlib.pyplot  as  pltlabels = place_sorted.index[:6]   #  取前六位values = place_sorted.values[:6]  #  取前六位plt.pie(calues,labels = labels,autopct=&quot;%.1f%%&quot;)plt.axis(&quot;equal&quot;)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;pre&gt;&lt;code&gt;# 1. 读取csv文件道程序中，用一个合适的变量存储它
# 2. 清洗，处理
# 3. 可视化，画图
# 安装 第三方库   jupyter
# series  一维数据,dataframe  二维数据（csv）
# 打开jupyter服务
import </summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="03_python数据分析" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/03-python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="课后" scheme="http://example.com/tags/%E8%AF%BE%E5%90%8E/"/>
    
  </entry>
  
  <entry>
    <title>python_tkinter_图形界面编程</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/02_python_tkinter/01_tkinter/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/02_python_tkinter/01_tkinter/</id>
    <published>2022-05-20T16:06:49.695Z</published>
    <updated>2022-05-20T16:42:32.993Z</updated>
    
    <content type="html"><![CDATA[<h2 id="系统登录"><a href="#系统登录" class="headerlink" title="系统登录"></a>系统登录</h2><pre><code># 安装  pip install tkinterfrom tkinter import *from tkinter import messageboxwindow = TK()window.title(&quot;灵犀教育管理系统&quot;)window.geometry(&quot;300x300+400+400&quot;)name_label = Lable(window,text = &quot;用户名&quot;)pwd_label = Lable(window,text = &quot;密码&quot;)name_entry=Entry(window)pwd_entry = Entry(window,show = &quot;*&quot;)uesrs = &#123;    &quot;admin&quot;:&quot;123&quot;&#125;def login()    # 获取用户名密码    name=name_entry.get()    pwd = pwd_entry.get()    if name ==&quot;&quot;:        messagebox.showerror(title= &quot;错误提示&quot;，message = &quot;用户名不能为空&quot;)    elif pwd ==&quot;&quot;:        messagebox.showerror(title= &quot;错误提示&quot;，message = &quot;密码不能为空&quot;)    else:        # 判断用户名是否存在，以及密码是否正确        if useer.get(name) == pwd:            messagebos.showinfo(title= &quot;提示&quot;，message = &quot;登陆成功&quot;)        else:            messagebox.showerror(title= &quot;错误提示&quot;，message = &quot;登陆失败&quot;)            login_btn = Button(window,text = &quot;登录&quot;，width = 8, command =login )#name_label.pack()  #  pack 居中顺序排列#pwd_label.pack()#name_entry.pack()#pwd_entry.pack()#login_btn.pack()# place(x,y)  #  详细位置布局# grid 表格形式name_label.grid(row = 0,column = 0,padx = 30,pady = 20)name_entry.grid(row = 0,column = 1)pwd_label.grid(row = 1,column = 0)pwd_entry.grid(row = 1,column = 1)login_btn.grid(row = 2,colimn = 1,pady = 20,stick = E)window.mainloop()</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;系统登录&quot;&gt;&lt;a href=&quot;#系统登录&quot; class=&quot;headerlink&quot; title=&quot;系统登录&quot;&gt;&lt;/a&gt;系统登录&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;# 安装  pip install tkinter
from tkinter import *
from t</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="02_python_tkinter" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/02-python-tkinter/"/>
    
    
    <category term="课后" scheme="http://example.com/tags/%E8%AF%BE%E5%90%8E/"/>
    
  </entry>
  
  <entry>
    <title>python第三方库-scrapy爬虫框架</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/01_python_scrapy%E6%A1%86%E6%9E%B6/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/01_python_scrapy%E6%A1%86%E6%9E%B6/</id>
    <published>2022-05-20T13:12:06.336Z</published>
    <updated>2022-05-23T07:30:52.268Z</updated>
    
    <content type="html"><![CDATA[<h1 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/">官方文档</a></h1><h2 id="爬虫原理"><a href="#爬虫原理" class="headerlink" title="爬虫原理"></a>爬虫原理</h2><h3 id="基础库的使用，"><a href="#基础库的使用，" class="headerlink" title="基础库的使用，"></a>基础库的使用，</h3><h3 id="进阶，"><a href="#进阶，" class="headerlink" title="进阶，"></a>进阶，</h3><h3 id="反爬虫，"><a href="#反爬虫，" class="headerlink" title="反爬虫，"></a>反爬虫，</h3><h3 id="单线程爬虫，"><a href="#单线程爬虫，" class="headerlink" title="单线程爬虫，"></a>单线程爬虫，</h3><h3 id="多线程爬虫，"><a href="#多线程爬虫，" class="headerlink" title="多线程爬虫，"></a>多线程爬虫，</h3><h3 id="scrapy（异步并发），"><a href="#scrapy（异步并发），" class="headerlink" title="scrapy（异步并发），"></a>scrapy（异步并发），</h3><h3 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h3><h2 id="框架："><a href="#框架：" class="headerlink" title="框架："></a>框架：</h2><h3 id="抓取，解析，保存"><a href="#抓取，解析，保存" class="headerlink" title="抓取，解析，保存"></a>抓取，解析，保存</h3><h3 id="域名，ip地址"><a href="#域名，ip地址" class="headerlink" title="域名，ip地址"></a>域名，ip地址</h3><h4 id="1-scrapy-异步并发-安装第三方库：pip-install-scrapy"><a href="#1-scrapy-异步并发-安装第三方库：pip-install-scrapy" class="headerlink" title="1. scrapy__异步并发 安装第三方库：pip install scrapy"></a>1. scrapy__异步并发 安装第三方库：pip install scrapy</h4><p>cmd 进入空白文件 通过scrapy 创建框架<br>    命令： </p><blockquote><p>scrapy startproject demo0520_scrapy（文件名,项目的名字）  回车<br><img src="/assets/2/20220520_1.jpg"><br>cd  demo0520_scrapy<br><img src="/assets/2/20220520_2.jpg"><br><img src="/assets/2/20220520_3.jpg"><br>scrapy genspider  movies（爬虫的类型，爬虫的名字） donban.com（域名）  # 需要加两个参数<br><img src="/assets/2/20220520_4.jpg"></p></blockquote><ol start="2"><li>打开pycharm<br> 打开项目：  项目位置</li></ol><p><img src="/assets/2/scrapy1.jpg"></p><ol start="3"><li>运行爬虫<br> 进入项目文件—&gt;&gt;&gt; cmd 或者pycharm 控制台<br> 输入 scrapy  crawl movies（爬虫的名字）</li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;官方文档&quot;&gt;&lt;a href=&quot;#官方文档&quot; class=&quot;headerlink&quot; title=&quot;官方文档&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://scrapy-chs.readthedocs.io/zh_CN/0.24/&quot;&gt;官方文档&lt;/a&gt;&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    <category term="scrapy框架" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/scrapy%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="测试高阶" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>python常用编码_整理</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/02_python%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A0%81/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/02_python%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A0%81/</id>
    <published>2022-05-19T06:43:08.855Z</published>
    <updated>2022-05-20T13:10:49.883Z</updated>
    
    <content type="html"><![CDATA[<pre><code># 去掉web页面自动化提醒标识- 自动化标识from selenium.webdriver.chrome.options import Optionoption = Options()option.add_experimental_option(&#39;excludeSwitches&#39;,[&#39;enable-automation&#39;])option.add_argument(&#39;--disable-blink-features-AutomationControlled&#39;)driver = webdriver.Chrome(options=option)</code></pre><pre><code># 关掉密码弹窗from selenium.webdriver.chrome.options import Optionprefs = &#123;&#125;prefs[&#39;credentials_enable_service&#39;] = Falseprefs[&#39;profile.password_manger_enabled&#39;] = Falseoption.add_experimental_option(&#39;prefs&#39;,prefs)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;pre&gt;&lt;code&gt;# 去掉web页面自动化提醒标识- 自动化标识
from selenium.webdriver.chrome.options import Option
option = Options()
option.add_experimental_option(&amp;#</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="课后" scheme="http://example.com/tags/%E8%AF%BE%E5%90%8E/"/>
    
  </entry>
  
  <entry>
    <title>测试高阶</title>
    <link href="http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    <id>http://example.com/05_%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/01_%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/</id>
    <published>2022-05-19T02:39:37.332Z</published>
    <updated>2022-05-20T15:38:59.103Z</updated>
    
    <content type="html"><![CDATA[<ul><li>爬虫技术接单平台<ul><li><a href="https://www.jfh.com/serviceProvider.html">解放号</a></li><li><a href="https://www.yuanjisong.com/job">猿急送</a></li><li><a href="https://www.proginn.com/?loginbox=show">程序员客栈</a></li><li><a href="https://codemart.com/">码市</a></li><li><a href="http://rrkf.com/">人人开发</a></li><li><a href="https://task.zbj.com/">猪八戒</a></li><li><a href="https://task.epwk.com/task/">一品威客</a></li><li><a href="https://zb.oschina.net/projects/list.html">开源众包</a></li><li><a href="https://www.taskcity.com/">智城外包网</a></li><li><a href="https://shixian.com/cases">实现网</a></li><li><a href="https://eleduck.com/categories/6/tags/0-18">电鸭社区</a></li><li><a href="https://remoteok.com/">Remoteok</a></li><li><a href="https://www.toptal.com/">Toptal</a></li><li><a href="https://angel.co/">AngelList</a></li><li><a href="https://www.yingxuan.io/">英选</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;爬虫技术接单平台&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.jfh.com/serviceProvider.html&quot;&gt;解放号&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.yuanjisong.com/job&quot;&gt;猿急送&lt;/</summary>
      
    
    
    
    <category term="05_测试高阶" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/"/>
    
    <category term="01_爬虫技术" scheme="http://example.com/categories/05-%E6%B5%8B%E8%AF%95%E9%AB%98%E9%98%B6/01-%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="课后" scheme="http://example.com/tags/%E8%AF%BE%E5%90%8E/"/>
    
  </entry>
  
  <entry>
    <title>python+requests+pytest接口自动化</title>
    <link href="http://example.com/07_%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/06_%E8%87%AA%E5%8A%A8%E5%8C%96/02_%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%9E%E6%93%8D/02_API%E8%87%AA%E5%8A%A8%E5%8C%96/api%E8%87%AA%E5%8A%A8%E5%8C%96%E6%A1%86%E6%9E%B6/03_python+requests+pytest%E6%8E%A5%E5%8F%A3%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    <id>http://example.com/07_%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/06_%E8%87%AA%E5%8A%A8%E5%8C%96/02_%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%9E%E6%93%8D/02_API%E8%87%AA%E5%8A%A8%E5%8C%96/api%E8%87%AA%E5%8A%A8%E5%8C%96%E6%A1%86%E6%9E%B6/03_python+requests+pytest%E6%8E%A5%E5%8F%A3%E8%87%AA%E5%8A%A8%E5%8C%96/</id>
    <published>2022-05-16T12:53:10.913Z</published>
    <updated>2022-05-16T13:01:43.017Z</updated>
    
    <content type="html"><![CDATA[<p>1、发送get请求</p><blockquote><p>#导包<br>import requests<br>#定义一个urlurl &#x3D; “<a href="http://xxxxxxx&quot;/">http://xxxxxxx&quot;</a><br>#传递参数payload&#x3D;”{&quot;head&quot;:{&quot;accessToken&quot;:&quot;&quot;,&quot;lastnotice&quot;:0,&quot;msgid&quot;:&quot;&quot;},&quot;body&quot;:{&quot;user_name&quot;:&quot;super_admin&quot;,&quot;password&quot;:&quot;b50c34503a97e7d0d44c38f72d2e91ad&quot;,&quot;role_type&quot;:1}}”<br>headers &#x3D; {‘Content-Type’: ‘text&#x2F;plain’,<br>‘Cookie’: ‘akpsysessionid&#x3D;bafc0ad457d5a99f3a4e53a1d4b32519’<br>}#发送get请求r &#x3D; requests.get( url&#x3D;url,headers&#x3D;headers, data&#x3D;payload)#打印结果print(r.text)<br>#解码print(r.encoding)<br>print(r.text.encode(‘utf-8’).decode(‘unicode_escape’))#先把返回的结果转换成utf-8，再去解码成中文的编码</p></blockquote><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><p>2、发送post请求</p><blockquote><p>#导包<br>import requests<br>#定义一个urlurl &#x3D; “<a href="http://xxxxxxx&quot;/">http://xxxxxxx&quot;</a><br>#传递参数payload&#x3D;”{&quot;head&quot;:{&quot;accessToken&quot;:&quot;&quot;,&quot;lastnotice&quot;:0,&quot;msgid&quot;:&quot;&quot;},&quot;body&quot;:{&quot;user_name&quot;:&quot;super_admin&quot;,&quot;password&quot;:&quot;b50c34503a97e7d0d44c38f72d2e91ad&quot;,&quot;role_type&quot;:1}}”<br>headers &#x3D; {‘Content-Type’: ‘text&#x2F;plain’,<br>‘Cookie’: ‘akpsysessionid&#x3D;bafc0ad457d5a99f3a4e53a1d4b32519’<br>}#发送post请求r &#x3D; requests.post( url&#x3D;url,headers&#x3D;headers, data&#x3D;payload)#打印结果print(r.text)</p></blockquote><h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><p>3、发送https请求</p><blockquote><p>import requests<br>url&#x3D;’<a href="https://www.ctrip.com/&#39;">https://www.ctrip.com/&#39;</a><br>#第一种解决方案，发送请求的时候忽略证书，证书的参数verify用的比较多r&#x3D;requests.post(url&#x3D;url,verify&#x3D;False)#verify参数默认为True，值为False，表示忽略证书#第二张解决方案，verify里面添加证书的路径r&#x3D;requests.post(url&#x3D;url,verify&#x3D;’证书的路径’)#verify参数默认为True，值为False，表示忽略证书<br>print(r.text)</p></blockquote><p>4、文件上传</p><blockquote><p>import requests<br>file &#x3D; {‘filename’:open(‘文件名称’,’rb’)<br>}response &#x3D; requests.post(“网址”,file)<br>print(response.text)</p></blockquote><p>5、文件下载</p><blockquote><p>#小文件下载<br>import requests<br>r &#x3D; requests.get(“<a href="https://img.sitven.cn/Tencent_blog_detail.jpg&quot;">https://img.sitven.cn/Tencent_blog_detail.jpg&quot;</a>)<br>with open(r”D:\a.jpg”, “wb”) as f:<br>f.write(r.content)#大文件下载import requests<br>def test_downloads(url, file):s &#x3D; requests.sessionr &#x3D; s.get(url, stream&#x3D;True, verify&#x3D;False)<br>with open(file, “wb”) as f:<br>for chunk in r.iter_content(chunk_size&#x3D;512):<br>f.write(chunk)if <strong>name</strong> &#x3D;&#x3D; “<strong>main</strong>“:<br>url &#x3D; “<a href="https://www.url.com/test/export&quot;">https://www.url.com/test/export&quot;</a><br>file &#x3D; “D:\a.xlsx”<br>test_downloads(url&#x3D;url, file&#x3D;file)#转载至：<a href="https://blog.csdn.net/weixin_43507959/article/details/107326912">https://blog.csdn.net/weixin_43507959/article/details/107326912</a></p></blockquote><p>6、timeout超时</p><blockquote><p>#导包<br>import requests<br>#循环10次<br>for i in range(0,10):<br>try:url&#x3D;”<a href="http://xxxxxxxxxxxxxxxx&quot;/">http://xxxxxxxxxxxxxxxx&quot;</a><br>data&#x3D;{“head”:{“lastnotice”:0,”msgid”:””,”accessToken”:”89a08bff-15d7-4d7a-9967-0b5f4fb699ce”},<br>“body”:{“clinicid”:”978f661e-1782-43bd-8675-b0ff1138ab7c”,”deptid”:”09b8515b-b01b-4771-9356-aed6b5aa01bf”,”doctorid”:”65ac0251-10ff-473a-af8a-20e8969176f7”,”registtype”:0,”card_num”:””,”bcc334”:””,”patientopt”:1,”bkc368”:”1”,”patient”:{“cardid”:””,”medicalcardid”:””,”label”:””,”sourcetype”:1,”nationid”:”01”,”maritalstatus”:0,”address”:””,”company”:””,”jobname”:””,”email”:””,”remark”:””,”bcc334”:””,”name”:”11”,”gender”:1,”phone”:””,”birthdate”:”2020-03-23”,”patienttype”:1,”szsbcardid”:””}}<br>}#发送post请求，超时时间0.03s<br>r&#x3D;requests.post(url&#x3D;url,json&#x3D;data,timeout&#x3D;0.03)<br>print(r.text)<br>print(r.cookies)<br>except:print(‘error’)<br>#可参考：<a href="https://blog.csdn.net/weixin_44350337/article/details/99655387">https://blog.csdn.net/weixin_44350337/article/details/99655387</a></p></blockquote><p>7、鉴权<br>7.1 auth参数鉴权</p><blockquote><p>import requests<br>url &#x3D; ‘<a href="http://192.168.1.1&/#39;">http://192.168.1.1&#39;</a><br>headers &#x3D; {} # 有的不带头也能请求到 不带头可以忽略这行 和headers&#x3D;headers,这两处r &#x3D; requests.get(url, auth&#x3D;(‘admin’, ‘123456’), headers&#x3D;headers, timeout&#x3D;10)<br>print(r.text)</p></blockquote><p>7.2 session操作</p><blockquote><p>#实例化session<br>session &#x3D; requests.session<br>#使用session发起请求<br>response &#x3D; session.post(url,headers&#x3D;req_header,data&#x3D;form_data)</p></blockquote><p>7.3 token操作</p><blockquote><p>import requests<br>url&#x3D;”<a href="http://xxxxxxxxxxxxxxx&quot;/">http://xxxxxxxxxxxxxxx&quot;</a><br>json&#x3D;{“head”:{“accessToken”:””,”lastnotice”:0,”msgid”:””},<br>“body”:{“username”:”15623720880”,”password”:”48028d2558577c526a017883211b4066”,”forceLogin”:0}<br>}r&#x3D;requests.post(url&#x3D;url,json&#x3D;json)print(r.text)<br>print(r.cookies)<br>#登录成功后返回token，带入下一个接口for i in range(0,1):<br>try:url&#x3D;”xxxxxxxxxxxxxxxxxx”<br>data&#x3D;{“head”:{“lastnotice”:0,”msgid”:””,”accessToken”:”89a08bff-15d7-4d7a-9967-0b5f4fb699ce”},<br>“body”:{“clinicid”:”978f661e-1782-43bd-8675-b0ff1138ab7c”,”deptid”:”09b8515b-b01b-4771-9356-aed6b5aa01bf”,”doctorid”:”65ac0251-10ff-473a-af8a-20e8969176f7”,”registtype”:0,”card_num”:””,”bcc334”:””,”patientopt”:1,”bkc368”:”1”,”patient”:{“cardid”:””,”medicalcardid”:””,”label”:””,”sourcetype”:1,”nationid”:”01”,”maritalstatus”:0,”address”:””,”company”:””,”jobname”:””,”email”:””,”remark”:””,”bcc334”:””,”name”:”11”,”gender”:1,”phone”:””,”birthdate”:”2020-03-23”,”patienttype”:1,”szsbcardid”:””}}<br>}r&#x3D;requests.post(url&#x3D;url,json&#x3D;data,timeout&#x3D;0.09)<br>print(r.text)<br>print(r.cookies)<br>except:print(‘error’)</p></blockquote><p>7.4 sign签名</p><blockquote><p>appid：wxd930ea5d5a258f4f</p><p>mch_id：10000100</p><p>device_info：1000</p><p>body：test</p><p>nonce_str：ibuaiVcKdpRxkhJA</p><p>import hashlib<br>#需要加密的字符串<br>stringA&#x3D;”appid&#x3D;wxd930ea5d5a258f4f&amp;body&#x3D;test&amp;device_info&#x3D;1000&amp;mch_id&#x3D;10000100&amp;nonce_str&#x3D;ibuaiVcKdpRxkhJA”;<br>#构建一个对象为md<br>md&#x3D;hashlib.md5<br>#对stringA字符串进行编码<br>md.update(stringA.encode)<br>#生成后的加密值<br>AES&#x3D;md.hexdigest<br>#把加密的结果，小写转大写 upper函数<br>AES&#x3D;AES.upper<br>print(AES)<br>参考微信支付：<a href="https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_3#">https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_3#</a></p></blockquote><h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><p>8、自动化模块划分<br>config 配置文件（python package）#directory和python package大同小异</p><p>common 公共的方法（python package）</p><p>testdata 测试数据（python package）</p><p>test_case测试用例（python package）</p><p>report 报告（directory）</p><p>run_case 测试执行（python package）</p><p>log 日志</p><p><img src="/assets/2/image-20220516205752-38m0prz.png" alt="image.png"></p><p>8.1 config配置文件</p><blockquote><p>def server_ip:<br>‘’’<br>ait_ip&#x3D;’’开发环境的服务器ip<br>sit_ip&#x3D;’’测试环境的服务器ip<br>:return: 返回不同服务器的地址<br>‘’’<br>server_add&#x3D;{‘dev_ip’ : ‘<a href="http://his.xxxxxxxxxxx.com&/#39;">http://his.xxxxxxxxxxx.com&#39;</a>,<br>‘sit_ip’ : ‘<a href="http://his.xxxxxxxxxxxx.comm&/#39;">http://his.xxxxxxxxxxxx.comm&#39;</a><br>}return server_add[‘dev_ip’]<br>————————————————————————————def sql_conf:<br>‘’’<br>host数据库ip<br>user数据库用户名<br>password数据库密码<br>database:连接数据库名<br>port数据库端口<br>chrset数据库字符集 中文utf-8<br>:return:<br>‘’’<br>host&#x3D;’localhost’<br>user&#x3D;’root’<br>password&#x3D;’123456’<br>database&#x3D;’mysql’<br>port&#x3D;3306<br>charset&#x3D;’utf8’ #这用utf8，utf-8会报错<br>return host,user,password,database,port,charset</p></blockquote><p>8.2 common 公共的方法</p><blockquote><p>封装一个读取Excel表格数据的函数</p><p>对Excel表格数据的读取需要用到一个库——xlrd库import xlrd</p><p>def get_excel_value(i):’’’<br>读取表中一行的数据<br>:return:返回2，3行数据<br>‘’’<br>filename &#x3D; r”..&#x2F;testdata&#x2F;jiekou.xls” #文件要用相对路径<br>book &#x3D; xlrd.open_workbook(filename) # 打开一个工作薄，不需要手动进行关闭# sheet &#x3D; book.sheet_by_name(“Sheet1”) 根据工作表的名字，获取一个工作表对象<br>sheet &#x3D; book.sheet_by_index(0) # 获取一个工作表，以index的方式，这里是获取第1个工作表<br>return sheet.cell_value(i,1),sheet.cell_value(i,2)</p><p>print(sheet.nrows) #打印所有行</p><p>print(sheet.ncols) #打印所有列</p><p>print(sheet.row_values(0)) #打印第一行</p><p>print(sheet.col_values(0)) #打印第一列</p><p>print(sheet.cell_value(0,1)) #打印第一行，第二列</p><p>for i in range(1, sheet.nrows):</p><p>print(sheet.cell_value(i,1),sheet.cell_value(i,2))# 打印单元格[所有数据]的值</p><p>str&#x3D;’(sheet.cell_value(i,1),sheet.cell_value(i,2)))’</p><p>print(str)</p><p>for i in range(1, sheet.nrows):</p><h1 id="for-j-in-range-0-sheet-ncols"><a href="#for-j-in-range-0-sheet-ncols" class="headerlink" title="for j in range(0, sheet.ncols):"></a>for j in range(0, sheet.ncols):</h1><p>print(sheet.cell_value(i,j)) # 打印单元格[i,j]的值</p><p>———————————————————————————————import pymysql<br>from config.sql_conf import *<br>def get_sql(sql):’’’<br>:param sql:运行查询的sql语句<br>:return:数据库查询结果<br>‘’’<br>#建立一个连接对象host, user, password, database, port, charset&#x3D;sql_confdb&#x3D;pymysql.connect(host&#x3D;host,user&#x3D;user,password&#x3D;password,database&#x3D;database,port&#x3D;port,charset&#x3D;charset)#建立一个游标cursor&#x3D;db.cursor#执行sql语句cursor.execute(sql)#把sql运行的数据保存在data变量里面data&#x3D;cursor.fetchall #获取查询出的所有的值cursor.close #关闭游标<br>db.close #关闭数据库连接<br>return data</p><p>print(get_sql(“SELECT help_topic_id FROM help_topic WHERE Name&#x3D;’MOD’”)) #执行sql语句</p><p>print(type(get_sql(“SELECT help_topic_id FROM help_topic WHERE Name&#x3D;’MOD’”)))</p></blockquote><p>8.3 estdata 测试数据</p><p>主要存放xls，txt，csv测试数据</p><p><img src="/assets/2/image-20220516210004-huegsxl.png" alt="image.png"></p><p>8.4 test_case测试用例</p><blockquote><p>from common.get_mysql import get_sql<br>from config.cof import server_ip<br>from common.get_excel import *from config.sql_conf import *<br>import requests# user_id&#x3D;get_sql(“SELECT help_topic_id FROM help_topic WHERE Name&#x3D;’MOD’”)#提取数据库数据</p><p>print(user_id)#打印结果</p><p>assert get_sql(“SELECT help_topic_id FROM help_topic WHERE Name&#x3D;’MOD’”)#断言数据库的数据是否存在</p><p>def test_aokao_login:url&#x3D;server_ip+’&#x2F;service&#x2F;user&#x2F;login’<br>username,password&#x3D;get_excel_value(1) #读取文件第二行数据<br>json&#x3D;{“head”:{“accessToken”:””,”lastnotice”:0,”msgid”:””},<br>“body”:{“username”:username,”password”:password,”forceLogin”:0}<br>}# usernamepassword&#x3D;get_excel_value(4)[0] #读取文件第二行数据</p><p>print(type(usernamepassword))</p><p>#把str类型转为字典格式 eval 函数# json&#x3D;eval(usernamepassword)r&#x3D;requests.post(url&#x3D;url,json&#x3D;json)print(r.text)</p><p>assert r.status_code&#x3D;&#x3D;200 #断言状态码是否等于200<br>assert ‘“accessToken”:”89a08bff-15d7-4d7a-9967-0b5f4fb699ce”,’ in r.text #断言返回信息是否包含accesstoken<br>def test_aokao_registadd:url &#x3D; server_ip+’&#x2F;service&#x2F;registration&#x2F;registadd’<br>data &#x3D; {“head”: {“lastnotice”: 0, “msgid”: “”, “accessToken”: “89a08bff-15d7-4d7a-9967-0b5f4fb699ce”},<br>“body”: {“clinicid”: “978f661e-1782-43bd-8675-b0ff1138ab7c”, “deptid”: “09b8515b-b01b-4771-9356-aed6b5aa01bf”,<br>“doctorid”: “65ac0251-10ff-473a-af8a-20e8969176f7”, “registtype”: 0, “card_num”: “”, “bcc334”: “”,<br>“patientopt”: 1, “bkc368”: “1”,<br>“patient”: {“cardid”: “”, “medicalcardid”: “”, “label”: “”, “sourcetype”: 1, “nationid”: “01”,<br>“maritalstatus”: 0, “address”: “”, “company”: “”, “jobname”: “”, “email”: “”,<br>“remark”: “”, “bcc334”: “”, “name”: “11”, “gender”: 1, “phone”: “”,<br>“birthdate”: “2020-03-23”, “patienttype”: 1, “szsbcardid”: “”}}<br>}r &#x3D; requests.post(url&#x3D;url, json&#x3D;data, timeout&#x3D;0.09)<br>print(r.text)<br>print(r.cookies)<br>assert r.status_code &#x3D;&#x3D; 200 # 断言状态码是否等于200</p></blockquote><p>8.5 report 报告</p><p>主要存放html，xml报告</p><p><img src="/assets/2/image-20220516210059-45ve97t.png" alt="image.png"></p><p>8.6 run_case 测试执行</p><blockquote><p>import pytest<br>‘’’<br>测试文件以test_开头，（以—_test结尾也可以）<br>测试类以Test开头，并且不能带有init 方法<br>测试函数以test_开头<br>断言使用基本的assert即可<br>‘’’<br>#如何去运行测试用例，_test开头的函数就可以，判断用例运行是否成功，assert断言if <strong>name</strong>&#x3D;&#x3D;”<strong>main</strong>“:<br>#单个文件运行，运行添加，对应的文件路径，路径要用相对路径# pytest.main([‘..&#x2F;test_case&#x2F;&#x2F;test_case_01.py’])<br>#多个文件运行，运行添加多个对应的文件路径，列表的形式，去添加多个文件的路径# pytest.main([‘..&#x2F;test_case&#x2F;test_fore.py’,’..&#x2F;test_case&#x2F;Dynamic correlation_token.py’])<br>#运行整个目录，添加目录的路径pytest.main([‘..&#x2F;test_case&#x2F;‘,’–html&#x3D;..&#x2F;report&#x2F;report.html’,’–junitxml&#x3D;..&#x2F;report&#x2F;report.xml’])<br>‘’’<br>pytest生成报告：<br>1、生成html报告<br>‘–html&#x3D;..&#x2F;report&#x2F;report.html’<br>2、生成xml报告<br>‘–junitxml&#x3D;..&#x2F;report&#x2F;report.xml’<br>‘’’</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、发送get请求&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;#导包&lt;br&gt;import requests&lt;br&gt;#定义一个urlurl &amp;#x3D; “&lt;a href=&quot;http://xxxxxxx&amp;quot;/&quot;&gt;http://xxxxxxx&amp;quot;&lt;/a&gt;&lt;br&gt;#</summary>
      
    
    
    
    <category term="07_测试工具" scheme="http://example.com/categories/07-%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/"/>
    
    <category term="06_自动化" scheme="http://example.com/categories/07-%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/06-%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    <category term="02_自动化实操" scheme="http://example.com/categories/07-%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/06-%E8%87%AA%E5%8A%A8%E5%8C%96/02-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%9E%E6%93%8D/"/>
    
    <category term="02_API自动化" scheme="http://example.com/categories/07-%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/06-%E8%87%AA%E5%8A%A8%E5%8C%96/02-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%9E%E6%93%8D/02-API%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    <category term="api自动化框架" scheme="http://example.com/categories/07-%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/06-%E8%87%AA%E5%8A%A8%E5%8C%96/02-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%9E%E6%93%8D/02-API%E8%87%AA%E5%8A%A8%E5%8C%96/api%E8%87%AA%E5%8A%A8%E5%8C%96%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="课后" scheme="http://example.com/tags/%E8%AF%BE%E5%90%8E/"/>
    
  </entry>
  
  <entry>
    <title>测试面试题精选【拼多多二面】</title>
    <link href="http://example.com/02_%E6%B5%8B%E8%AF%95%E7%90%86%E8%AE%BA/10_%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E9%A2%98%E5%88%86%E6%9E%90/09_%E6%B5%8B%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89%E3%80%90%E6%8B%BC%E5%A4%9A%E5%A4%9A%E4%BA%8C%E9%9D%A2%E3%80%91/"/>
    <id>http://example.com/02_%E6%B5%8B%E8%AF%95%E7%90%86%E8%AE%BA/10_%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E9%A2%98%E5%88%86%E6%9E%90/09_%E6%B5%8B%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89%E3%80%90%E6%8B%BC%E5%A4%9A%E5%A4%9A%E4%BA%8C%E9%9D%A2%E3%80%91/</id>
    <published>2022-05-16T12:29:03.176Z</published>
    <updated>2022-05-16T12:36:55.498Z</updated>
    
    <content type="html"><![CDATA[<p>面试一般分为技术面和hr面，形式的话很少有群面，少部分企业可能会有一个交叉面，不过总的来说，技术面基本就是考察你的专业技术水平的，hr面的话主要是看这个人的综合素质以及家庭情况符不符合公司要求，一般来讲，技术的话只要通过了技术面hr面基本上是没有问题（也有少数企业hr面会刷很多人）</p><p><img src="/assets/2/image-20220516203639-zpdniqr.png" alt="image.png"></p><p>我们主要来说技术面，技术面的话主要是考察专业技术知识和水平，下面是我们整理好的自动化测试岗的面试题。</p><p>1.如何把自动化测试在公司中实施并推广起来的？<br>1.项目组调研选择自动化工具并开会演示demo案例，我们主要是演示selenium和robotframework两种。</p><p>2.搭建自动化测试框架，在项目中逐步开展自动化。</p><p>3.把该项目的自动化流程、框架固化成文档</p><p>4.推广到公司的其它项目组应用</p><p>2.请描述一下自动化测试流程？<br>1.编写自动化测试计划</p><p>2.设计自动化测试用例</p><p>3.编写自动化测试框架和脚本</p><p>4.调试并维护脚本</p><p>5.无人值守测试</p><p>6.后期脚本维护（添加用例、开发更新版本）</p><p>3.自动化测试用例如何编写？以下答案二选一即可：<br>1.用例是自动化测试工程师自己设计的，一般刚开始已基本业务流程为主（登录–完成一个业务–退出）</p><p>2.从系统测试用例中进行筛选或由业务工程师提供</p><p>4.上一个项目中自动化测试的执行策略？<br>上一个项目中是定时执行的，设置的执行时间是晚上12点，执行完毕后会自动发送邮件通知</p><p>5.自动化测试发现BUG多吗？<br>不多，因为之前项目组是把已经测试通过的基本功能再进行自动化脚本编写和在后续版本执行自动化测试，它主要是保证已经测试通过的功能在新版本更新后没有问题。</p><p>6.你觉得自动化测试的价值在哪里？你们公司为什么要做自动化测试？<br>引用自动化测试之后，能代替大量繁琐的回归测试工作，把业务测试人员解放出来，既而让业务测试人员把精力集中在复杂的业务功能模块上，自动化测试一般是对稳定下来的功能进行自动化，保证不会因为产品的更新导致之前稳定下来的功能出现BUG</p><p>7.自动化测试有误报过bug吗？产生误报怎么办？<br>有误报过，有时候自动化测试报告中显示发现了bug,实际去通过手工测试去确认又不存在该bug。</p><p>误报原因一般是：</p><p>1.元素定位不稳定，需要尽量提高脚本的稳定性；</p><p>2.开发更新了页面但是测试没有及时更新维护!</p><p>8.自动化测试过程中，你遇到了哪些问题，是如何解决的？<br>1.频繁地变更页面，经常要修改页面对象类里面的代码</p><p>2.自动化测试偶尔出现过误报</p><p>3.自动化测试结果出现覆盖的情况：Jenkins根据时间建立文件夹</p><p>4.自动化测试代码维护比较麻烦</p><p>5.自动化测试进行数据库对比数据</p><p>9.在上一家公司做自动化测试用的什么框架？<br>可以说出以下自己擅长的一种：</p><p>1.python+selenium+unittest+htmltestrunner</p><p>2.python+selenium+pytest+allure</p><p>robotframework+Selenium3<br>10.在selenium自动化测试中，你一般完成什么类型的测试？自动化覆盖率？<br>主要是冒烟测试和回归测试。回归测试主要写一些功能稳定的场景，通过自动化手段去实现，节约测试时间。因为自动化测试用例也是在不断的更新和迭代，没有刻意去统计，大概在30%-40%左右！</p><p>11.在执行脚本过程，如何实现当前元素高亮显示？<br>这个其实就是利用javaScript去修改当前元素的边框样式来到达高亮显示的效果，</p><p>12.如果一个元素无法定位，你一般会考虑哪些方面的原因？<br>1.页面加载元素过慢，加等待时间</p><p>2.页面有frame框架页，需要先跳转入frame框架再定位</p><p>3.可能该元素是动态元素，定位方式要优化，可以使用部分元素定位或通过父节点或兄弟节点定位。</p><p>4.可能识别了元素，但是不能操作，比如元素不可用，不可写等。需要使用js先把前置的操作完成，</p><p>13.元素定位方法你熟悉的有哪些？（八大元素定位方式）<br>id ，name， class， tag， link_text， Partial link text， css， xpath</p><p>14.遇到frame框架页面怎么处理？<br>先用driver.switch_to.frame()跳转进去frame，</p><p>然后再操作页面元素，</p><p>操作完后使用driver.swith_to.default_content()跳转出来</p><p>15.遇到alert弹出窗如何处理？<br>使用driver.switch_to.alert方法先跳转到alert弹出窗口</p><p>然后再通过accept点击确定按钮，通过dismiss点击取消难，通过text()获得弹出窗口的文本。</p><p>16.如何处理多窗口？<br>这个多窗口之间跳转处理，我们在项目中也经常遇到。就是，当你点击一个链接，这个链接会在一个新的tab打开，然后你接下来要在新tab打开的页面查找元素，</p><p>1.我们在点击链接前使用driver.current_window_handle获得当前窗口句柄。</p><p>2.再点击链接。点击后通过driver.window_handles获得所有窗口的句柄，</p><p>3.然后再循环找到新窗口的句柄，然后再通过driver.switch_to.window()方法跳转到新的窗口。</p><p>17.怎么验证元素是enable&#x2F;disabled&#x2F;checked状态？<br>定位元素后：分别通过isEnabled()，isSelected()，isDisplayed()三个方法进行判断。</p><p>18.如何处理下拉菜单？<br>在Selenium中有一个叫Select的类，这个类支持对下拉菜单进行操作。使用方法如下：</p><p>1.定位元素</p><p>2.把定位的元素转化成Select对象。</p><p>sel &#x3D; Select(定位的元素对象)</p><p>3.通过下标或者值或者文本选中下拉框。</p><p>sel.select_by_index(index);<br>sel.select_by_value(value);<br>sel.select_by_visible_text(text);</p><p>19.在日历这种web 表单你是如何处理的?<br>首先要分析当前网页试用日历插件的前端代码，看看能不能通过元素定位，点击日期实现，如果不能，可能需要借助javascript。还有些日历控件一个文本输入框，可以直接sendKeys()方法来实现传入一个时间的数据。</p><p>20.举例一下说明一下你遇到过那些异常<br>常见的selenium异常有这些：</p><p>NoSuchElementException：没有该元素异常<br>TimeoutException ：超时异常</p><p>ElementNotVisibleException ：元素不可见异常<br>NoSuchAttributeException ：没有这样属性异常<br>NoSuchFrameException ：没有该frame异常</p><p>21.关闭浏览器中quit和close的区别<br>简单来说，两个都可以实现退出浏览器session功能，close是关闭你当前聚焦的tab页面，而quit是关闭全部浏览器tab页面，并退出浏览器session。知道这两个区别，我们就知道quit一般用在结束测试之前的操作，close用在执行用例过程中关闭某一个页面的操作。</p><p>22.在Selenium中如何实现截图，如何实现用例执行失败才截图<br>在Selenium中提供了一个get_screenshot_as_file()的方法来截图的，一般结合try&#x2F;except捕获异常时使用，进行错误截图。</p><p>23.如何实现文件上传？<br>定位元素后，直接使用send_keys()方法设置就行，参数为需要上传的文件的路径。</p><p>24.自动化中有哪三类等待？他们有什么特点？<br>1.线程等待（强制等待）如time.sleep(2)：线程强制休眠2秒钟，2秒过后，再执行后续的代码。建议少用。</p><p>2.imlicitlyWait（隐式等待）会在指定的时间范围内不断的查找元素，直到找到元素或超时，特点是必须等待整个页面加载完成。</p><p>3.WebDriverWait（显式等待）通常是我们自定义的一个函数代码，这段代码用来等待某个元素加载完成，再继续执行后续的代码</p><p>25.你写的测试脚本能在不同浏览器上运行吗<br>当然可以，我写的用例可以在在IE，火狐和谷歌这三种浏览器上运行。实现的思路是封装一个方法，分别传入一个浏览器的字符串，如果传入IE就使用IE，如果传入FireFox就使用FireFox，如果传入Chrome就使用Chrome浏览器，并且使用什么浏览器可以在总的ini配置文件中进行配置。需要注意的是每个浏览器使用的驱动不一样。</p><p>26.什么是PO模式，为什么要使用它<br>PO是Page Object 模式的简称，它是一种设计思想，意思是，把一个页面，当做一个对象，页面的元素和元素之间操作方法就是页面对象的属性和行为，PO模式一般使用三层架构，分别为：基础封装层BasePage，PO页面对象层，TestCase测试用例层。</p><p>27.你会封装自动化测试框架吗？<br>这个问得最多，甚至有很多公司直接卸载招聘要求中</p><p>当然可以，自动化框架主要的核心框架就是分层+PO模式：分别为：基础封装层BasePage，PO页面对象层，TestCase测试用例层。然后再加上日志处理模块，ini配置文件读取模块，unittest+ddt数据驱动模块，jenkins持续集成模式组成。</p><p>28.你们测试团队如何提升自己的测试技能？<br>更多是靠技术讨论和学习交流，除了我们公司内部群之外，我们还会有相关的技术交流群，可以和跟很多同行一起学习，完善自己的技能树。IT行业技术更新迭代本来就很快，所以更需要保持学习的心态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;面试一般分为技术面和hr面，形式的话很少有群面，少部分企业可能会有一个交叉面，不过总的来说，技术面基本就是考察你的专业技术水平的，hr面的话主要是看这个人的综合素质以及家庭情况符不符合公司要求，一般来讲，技术的话只要通过了技术面hr面基本上是没有问题（也有少数企业hr面会刷</summary>
      
    
    
    
    <category term="02_测试理论" scheme="http://example.com/categories/02-%E6%B5%8B%E8%AF%95%E7%90%86%E8%AE%BA/"/>
    
    <category term="10_模拟面试题分析" scheme="http://example.com/categories/02-%E6%B5%8B%E8%AF%95%E7%90%86%E8%AE%BA/10-%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E9%A2%98%E5%88%86%E6%9E%90/"/>
    
    
    <category term="测试理论" scheme="http://example.com/tags/%E6%B5%8B%E8%AF%95%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
</feed>
